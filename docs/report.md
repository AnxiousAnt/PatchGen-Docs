# GSoC'24 Final Project Report 
| | |
|------------------------|--------------------------------------------|
| **Name**               | Amrut Kotrannavar                          |
| **Organization**       | Purr Data                                  |
| **Project Title**      | Generating Pd patches using an LLM          |
| **Project Duration**   | 22 Weeks                                   |
| **Mentors**            | Jonathan Wilkes, Albert Gräf, Matt Barber  |
|||

# Project Overview

The objective of the project, **"Generating Purr Data (Pd) Patches Using an LLM"** was to employ an open-source language model to generate Pd patches from natural language inputs. This aims to simplify the patch creation process in Pd, a visual programming language for multimedia projects.

# Documentation

For detailed project documentation visit [PatchGen Docs](https://anxiousant.github.io/PatchGen-Docs/#/).

## Current Model Performance

The model demonstrates a solid grasp of Pd syntax and is able to generate some simple patches, but it faces challenges in accurately converting natural language descriptions into functional patches. This limitation is primarily due to the limited number of annotated examples, leading to some inconsistencies when handling complex or abstract prompts.

## Code Integration

The project was an independent effort, and no code was merged upstream into existing repositories. The entire work remains standalone and fully open-source, allowing for future collaboration and expansion by the community.

## Future Work

- **Further Training:** The model requires additional fine-tuning on a broader and more diverse set of annotated examples to improve its ability to handle a wider range of natural language descriptions and generate more accurate patches.

- **Community Engagement:** Establishing an online forum would enable users to share successful prompts, discuss challenges, and contribute new examples to expand the training dataset. This forum could also be a hub for collecting more training data, which would help further improve the model’s performance.

## Acknowledgements

I had a great time working on this project and I’d like to sincerely thank my mentors Jonathan Wilkes, Albert Gräf and Matt Barber for their support throughout the course of the project. I’m truly grateful for their time, feedback, and encouragement, which helped me achieve my goals and learn a ton along the way.


